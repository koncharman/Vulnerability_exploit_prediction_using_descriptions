###Load a json file from NVD data feeds
#In our study we employed the following script for the 2022 NVD datafeed
data_files=jsonlite::read_json(paste(getwd(),"/nvdcve_2022/nvdcve_2022.json",sep=""))
data_files$CVE_Items=append(data_files$CVE_Items,d_old$CVE_Items)

##reject entries without cvss and entries that are rejected by NVD
cve_titles=unlist((lapply(data_files$CVE_Items,function(x)
  x[["cve"]][["description"]][["description_data"]][[1]][["value"]])))#

cvss_base_score=unlist(lapply(data_files$CVE_Items,function(x)length(x[["impact"]]
                                                                     [["baseMetricV2"]][["cvssV2"]][["baseScore"]])))


reject_entries=grep("*REJECT",cve_titles)
reject_entries=unique(append(reject_entries,which(cvss_base_score==0)))

data_files$CVE_Items=data_files$CVE_Items[-reject_entries]



#### assign features

#Titles
cve_titles=unlist((lapply(data_files$CVE_Items,function(x)
  x[["cve"]][["description"]][["description_data"]][[1]][["value"]])))#

#CVSS
cvss_base_score=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]]
                              [["baseMetricV2"]][["cvssV2"]][["baseScore"]]))

#CWE codes
cwe_info_codes=lapply(data_files$CVE_Items,function(x)
  x[["cve"]][["problemtype"]][["problemtype_data"]][[1]]$description)

cwe_info_codes=lapply(cwe_info_codes,function(x)unique(unlist(lapply(x,function(y)y$value))))

cve_id=unlist(lapply(data_files$CVE_Items,function(x)x[["cve"]][["CVE_data_meta"]][["ID"]]))


#Records that have exploits
has_exploit=unlist(lapply(data_files$CVE_Items,function(x){
  all_tags=unlist(lapply(x$cve$references$reference_data,function(y)y$tags))
  
  return("Exploit"%in%all_tags)
  
}))


####6 severity metrics (CVSS v2)
factors=list()
factors[['acess_vector']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["accessVector"]]))
factors[['accessComplexity']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["accessComplexity"]]))
factors[['authentication']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["authentication"]]))
factors[['confidentialityImpact']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["confidentialityImpact"]]))
factors[['integrityImpact']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["integrityImpact"]]))
factors[['availabilityImpact']]=unlist(lapply(data_files$CVE_Items,function(x)x[["impact"]][["baseMetricV2"]][["cvssV2"]][["availabilityImpact"]]))



####


##Fixes in cwe
nocwe_pos=which(unlist(lapply(cwe_info_codes,function(x)length(x)))==0)
cwe_info_codes[nocwe_pos]="NVD-CWE-noinfo"

ml_cwes=which(unlist(lapply(cwe_info_codes,function(x)length(x)))>1)

cwe_info_codes=as.matrix(cwe_info_codes)
##



df=data.frame("CVE_TITLE"=cve_titles,"CWE_CODE"=cwe_info_codes,"has_exploit"=has_exploit,"cvss_base_score"=cvss_base_score,"id"=cve_id)
df$has_exploit[df$has_exploit==T]=1
df$has_exploit[df$has_exploit==F]=0

####6 factors
for(i in 1:length(factors)){
  df=cbind(df,factors[[i]])
}
colnames(df)[(ncol(df)-5):ncol(df)]=
  c("acess_vector","accessComplexity","authentication","confidentialityImpact","integrityImpact","availabilityImpact")
####

##Save it as df_exploit



######Text Preprocessing
# Data Wrangling
library(tidyverse)

# Text Preprocessing
library(tidytext)
library(textclean)
library(furrr) 
plan(multisession, workers = 4) # Using 4 CPU cores

####Text preprocessing function
cleansing_text <- function(x) x %>% 
  replace_non_ascii() %>% 
  tolower() %>% 
  str_replace_all(pattern = "\\@.*? |\\@.*?[:punct:]", replacement = " ") %>% 
  #str_remove(pattern = "early access review") %>%
  replace_url() %>% 
  replace_hash() %>% 
  replace_html() %>% 
  replace_contraction() %>% 
  replace_word_elongation() %>% 
  str_replace_all("\\?", " questionmark") %>% 
  str_replace_all("\\!", " exclamationmark") %>% 
  str_replace_all("[:punct:]", " ") %>% 
  str_replace_all("[:digit:]", " ") %>% 
  str_trim() %>% 
  str_squish()

######

###OLD entries - Need to download as well - in our study we dowloaded from 2015 to 2021

library(jsonlite)
d_old=read_json("nvdcve_2015.json")


for(i in 2016:2021){
  print(i)
  dir_temp=paste("nvdcve_",i,".json",sep="")
  d_old$CVE_Items=append( d_old$CVE_Items,read_json(dir_temp)$CVE_Items)
}

all_set_text=unlist((lapply(d_old$CVE_Items,function(x)
  x[["cve"]][["description"]][["description_data"]][[1]][["value"]])))#

reject_entries=grep("*REJECT",all_set_text)
all_set_text=all_set_text[-reject_entries]
all_set_text=unique(all_set_text)

write.csv(all_set_text,"all_set_text_cve_2015_2021.csv")
#
##



all_set_text=read.csv("all_set_text_cve_2015_2021.csv")
all_set_text=all_set_text$x

library(tm)
corpus = Corpus(VectorSource(all_set_text))
corpus = tm_map(corpus, cleansing_text)
corpus = tm_map(corpus, removeWords,  c("the",stopwords("english")))#Glove

corpus = tm_map(corpus, stemDocument)

min_doc=length(all_set_text)*0.002 # 0.1
max_doc=length(all_set_text)*0.5 # 0.05
frequencies=DocumentTermMatrix(corpus, control = list(bounds = list(global = c(min_doc, max_doc))))

#The data strucrutes in the repository are constructed as follows 
tSparse = as.data.frame(as.matrix(frequencies))
tcm_train=crossprod(binda::dichotomize(tSparse,1))
tSparse_colnames=colnames(tSparse)

####

####Load 2022 entries
tSparse_colnames=readRDS("tSparse_colnames")

df_exploit=readRDS("df_exploit")
all_set_text=df_exploit[,1]

####for pre processing see above
frequencies=DocumentTermMatrix(corpus)

matched_names=match(tSparse_colnames,frequencies$dimnames$Terms)
no_matched_no=which(is.na(matched_names))

no_matched_names=tSparse_colnames[-which(is.na(matched_names))]

tSparse = as.data.frame(as.matrix(frequencies[,no_matched_names]))

for(i in 1:length(no_matched_no)){
  tSparse=cbind(tSparse,0)
  colnames(tSparse)[ncol(tSparse)]=tSparse_colnames[no_matched_no[i]]
}

tSparse=tSparse[,tSparse_colnames]

#this is how tSparse_2022 is constructed

#########For the training tasks you will need the following files as constructed before

df_exploit=readRDS("df_exploit")

tSparse=readRDS("tSparse_2022")

#### training and testing datasets - 70% training dataset (TRUE values) - 30% testing dataset (FALSE values)
split2=read.csv("split2_binary_categories_assignement.csv")
split2$X=NULL

categories_assignement=as.numeric(df_exploit[,2])






